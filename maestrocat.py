#!/usr/bin/env python3
"""
MaestroCat Universal Launcher
Automatically detects platform and starts the appropriate agent configuration.
"""

import asyncio
import platform
import subprocess
import sys
import os
from pathlib import Path

def detect_platform():
    """Detect the current platform and return the appropriate configuration"""
    system = platform.system().lower()
    
    if system == "darwin":  # macOS
        return "macos"
    elif system == "linux":
        # Check if running in WSL
        try:
            with open("/proc/version", "r") as f:
                version_info = f.read().lower()
                if "microsoft" in version_info or "wsl" in version_info:
                    return "wsl"
        except FileNotFoundError:
            pass
        return "linux"
    else:
        return "unknown"

def check_gpu_availability():
    """Check if NVIDIA GPU is available for Docker"""
    try:
        result = subprocess.run(["nvidia-smi"], capture_output=True, check=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def pull_docker_images(platform_type):
    """Pull the appropriate Docker images based on platform"""
    images_to_pull = []
    
    if platform_type == "macos":
        # Skip WhisperLive on macOS since it doesn't support ARM64
        # macOS users should use native Whisper.cpp instead
        images_to_pull = [
            ("ghcr.io/remsky/kokoro-fastapi-cpu:latest", "Kokoro TTS CPU"),
            ("ollama/ollama:latest", "Ollama LLM")
        ]
        print("ğŸ“¥ Pulling ARM64-compatible images for macOS...")
        print("âš ï¸  Note: WhisperLive skipped (no ARM64 support). Use native Whisper.cpp instead.")
    else:
        # Linux/WSL - check for GPU
        if check_gpu_availability():
            images_to_pull = [
                ("ghcr.io/collabora/whisperlive-gpu:latest", "WhisperLive GPU"),
                ("ghcr.io/remsky/kokoro-fastapi-gpu:latest", "Kokoro TTS GPU"),
                ("ollama/ollama:latest", "Ollama LLM")
            ]
            print("ğŸ“¥ Pulling GPU-accelerated images...")
        else:
            images_to_pull = [
                ("ghcr.io/collabora/whisperlive-cpu:latest", "WhisperLive CPU"),
                ("ghcr.io/remsky/kokoro-fastapi-cpu:latest", "Kokoro TTS CPU"),
                ("ollama/ollama:latest", "Ollama LLM")
            ]
            print("ğŸ“¥ Pulling CPU-only images...")
    
    for image, name in images_to_pull:
        try:
            # Check if image already exists
            result = subprocess.run(
                ["docker", "images", "-q", image], 
                capture_output=True, text=True
            )
            if result.stdout.strip():
                print(f"âœ… {name} image already available")
                continue
            
            # Pull the image
            print(f"â¬‡ï¸  Downloading {name}: {image}")
            result = subprocess.run(
                ["docker", "pull", image], 
                capture_output=True, text=True
            )
            if result.returncode == 0:
                print(f"âœ… {name} downloaded successfully")
            else:
                print(f"âŒ Failed to pull {name}: {result.stderr}")
                return False
        except Exception as e:
            print(f"âŒ Error pulling {name}: {e}")
            return False
    
    return True

def start_docker_services(platform_type):
    """Start all Docker services based on platform"""
    # First, ensure we have all required images
    if not pull_docker_images(platform_type):
        print("âŒ Could not download required images")
        return False
    
    if platform_type == "macos":
        print("ğŸ Starting macOS-specific Docker services...")
        print("ğŸ“¦ Services: Ollama + Kokoro (CPU-optimized, WhisperLive excluded)")
        cmd = ["docker-compose", "-f", "docker-compose.macos.yml", "up", "-d"]
    else:
        # Linux/WSL - check for GPU
        if check_gpu_availability():
            print("ğŸš€ Starting GPU-accelerated Docker services for Linux/WSL...")
            print("ğŸ“¦ Services: WhisperLive + Ollama + Kokoro (all GPU-accelerated)")
            cmd = ["docker-compose", "-f", "docker-compose.yml", "-f", "docker-compose.gpu.yml", "up", "-d"]
        else:
            print("ğŸ’» Starting CPU-only Docker services for Linux/WSL...")
            print("ğŸ“¦ Services: WhisperLive + Ollama + Kokoro (all CPU-only)")
            cmd = ["docker-compose", "-f", "docker-compose.yml", "-f", "docker-compose.cpu.yml", "up", "-d"]
    
    try:
        print(f"ğŸ”„ Running: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… Docker services started successfully")
            if platform_type != "macos":
                print("ğŸ”Š WhisperLive STT: ws://localhost:9090")
            print("ğŸ§  Ollama LLM: http://localhost:11434")
            if platform_type == "macos":
                print("ğŸ—£ï¸  Kokoro TTS: http://localhost:5001")
                print("ğŸŒ Kokoro Web UI: http://localhost:5001/web")
            else:
                print("ğŸ—£ï¸  Kokoro TTS: http://localhost:5000")
                print("ğŸŒ Kokoro Web UI: http://localhost:5000/web")
            return True
        else:
            print(f"âŒ Failed to start Docker services (exit code {result.returncode})")
            if result.stderr:
                print(f"STDERR: {result.stderr}")
            if result.stdout:
                print(f"STDOUT: {result.stdout}")
            return False
    except Exception as e:
        print(f"âŒ Error starting Docker services: {e}")
        return False

def check_dependencies(platform_type):
    """Check if required dependencies are available for the platform"""
    missing = []
    
    # All platforms now use Docker for consistency
    try:
        result = subprocess.run(["docker", "--version"], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        missing.append("Docker (install from: https://docker.com)")
    
    try:
        result = subprocess.run(["docker-compose", "--version"], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        missing.append("Docker Compose (install with: pip install docker-compose)")
    
    # Check if Docker daemon is running
    try:
        result = subprocess.run(["docker", "ps"], capture_output=True, check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        missing.append("Docker daemon not running (start Docker Desktop or run: sudo systemctl start docker)")
    
    return missing

def get_config_and_example(platform_type):
    """Get the appropriate config file and example script for the platform"""
    if platform_type == "macos":
        return "config/maestrocat_macos.yaml", "examples/local_maestrocat_macos.py"
    else:
        return "config/maestrocat.yaml", "examples/local_maestrocat_agent.py"

def print_platform_info(platform_type, config_file, example_file):
    """Print platform-specific information"""
    print("ğŸ­ MaestroCat Universal Launcher")
    print("=" * 50)
    
    if platform_type == "macos":
        print("ğŸ Detected: macOS (Apple Silicon optimized)")
        print("ğŸš€ Using: Native services (Whisper.cpp + Ollama + macOS TTS)")
        print("âš¡ Performance: Metal acceleration enabled")
    elif platform_type == "wsl":
        print("ğŸ§ Detected: Windows Subsystem for Linux")
        print("ğŸ³ Using: Docker services (WhisperLive + Ollama + Kokoro)")
        print("ğŸ”Š Audio: WSL audio transport")
    elif platform_type == "linux":
        print("ğŸ§ Detected: Linux")
        print("ğŸ³ Using: Docker services (WhisperLive + Ollama + Kokoro)")
        print("ğŸµ Audio: PyAudio transport")
    elif platform_type == "windows":
        print("ğŸªŸ Detected: Windows")
        print("ğŸ³ Using: Docker services (WhisperLive + Ollama + Kokoro)")
        print("ğŸµ Audio: Windows audio transport")
    else:
        print(f"â“ Detected: {platform_type} (unknown)")
        print("ğŸ³ Using: Docker services (fallback)")
    
    print("")
    print(f"ğŸ“„ Config: {config_file}")
    print(f"ğŸ¯ Script: {example_file}")
    print("=" * 50)

def print_setup_instructions(platform_type, missing_deps):
    """Print setup instructions for missing dependencies"""
    if not missing_deps:
        return
        
    print("âŒ Missing Dependencies:")
    for dep in missing_deps:
        print(f"   â€¢ {dep}")
    print("")
    
    if platform_type == "macos":
        print("ğŸ”§ macOS Setup Commands:")
        print("   brew install ollama whisper-cpp ffmpeg")
        print("   ollama serve &")
        print("   ollama pull llama3.2:3b")
        print("")
    else:
        print("ğŸ”§ Docker Setup Commands:")
        print("   docker-compose up -d")
        print("   docker-compose ps  # Check services")
        print("")

async def main():
    """Main launcher function"""
    # Detect platform
    platform_type = detect_platform()
    
    # Get appropriate configuration
    config_file, example_file = get_config_and_example(platform_type)
    
    # Print platform info
    print_platform_info(platform_type, config_file, example_file)
    
    # Check dependencies
    missing_deps = check_dependencies(platform_type)
    
    if missing_deps:
        print_setup_instructions(platform_type, missing_deps)
        print("â— Please install missing dependencies and try again.")
        return 1
    
    # Check if config and example files exist
    if not os.path.exists(config_file):
        print(f"âŒ Config file not found: {config_file}")
        return 1
        
    if not os.path.exists(example_file):
        print(f"âŒ Example file not found: {example_file}")
        return 1
    
    print("âœ… All dependencies available!")
    print("")
    
    # Start all Docker services for consistency across platforms
    print("ğŸ”§ Starting Docker services...")
    if not start_docker_services(platform_type):
        print("â— Warning: Could not start Docker services. Please check Docker installation.")
    
    print("")
    print("ğŸš€ Starting MaestroCat...")
    print(f"ğŸ’» Platform: {platform_type}")
    
    # All platforms now use Docker for consistency
    if platform_type == "macos":
        print("ğŸ¤ STT: Native Whisper.cpp (Apple Silicon optimized)")
        print("ğŸ§  LLM: Ollama (Docker, CPU-optimized)")  
        print("ğŸ—£ï¸  TTS: Kokoro (Docker, CPU-optimized)")
    elif check_gpu_availability():
        print("ğŸ¤ STT: WhisperLive (Docker, GPU-accelerated)")
        print("ğŸ§  LLM: Ollama (Docker, GPU-accelerated)")
        print("ğŸ—£ï¸  TTS: Kokoro (Docker, GPU-accelerated)")
    else:
        print("ğŸ¤ STT: WhisperLive (Docker, CPU-only)")
        print("ğŸ§  LLM: Ollama (Docker, CPU-only)")
        print("ğŸ—£ï¸  TTS: Kokoro (Docker, CPU-only)")
        
    print("ğŸŒ WebSocket: http://localhost:8765/ws")
    print("ğŸ› Debug UI: http://localhost:8080")
    
    print("")
    print("Press Ctrl+C to stop")
    print("=" * 50)
    
    # Import and run the appropriate example
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    
    try:
        if platform_type == "macos":
            from examples.local_maestrocat_macos import MacOSMaestroCatAgent
            agent = MacOSMaestroCatAgent(config_file)
        else:
            from examples.local_maestrocat_agent import LocalMaestroCatAgent
            agent = LocalMaestroCatAgent(config_file)
        
        await agent.run()
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ MaestroCat shutting down...")
        return 0
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        return 1

def cli():
    """Command line interface"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="MaestroCat Universal Launcher",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    maestrocat                    # Auto-detect platform and start
    python maestrocat.py          # Same as above
    python -m maestrocat          # Module execution
    
Platform Detection:
    â€¢ macOS: Uses native Whisper.cpp + Ollama + macOS TTS
    â€¢ Linux: Uses Docker services (WhisperLive + Ollama + Kokoro)
    â€¢ WSL: Uses Docker services with WSL audio transport
    â€¢ Windows: Uses Docker services with Windows audio transport
        """
    )
    
    parser.add_argument(
        "--platform", 
        choices=["macos", "linux", "wsl", "windows"],
        help="Force specific platform instead of auto-detection"
    )
    
    parser.add_argument(
        "--config",
        help="Path to custom config file"
    )
    
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="Only check dependencies, don't start the agent"
    )
    
    parser.add_argument(
        "--start-docker",
        action="store_true",
        help="Only start Docker services, don't run the agent"
    )
    
    args = parser.parse_args()
    
    # Override platform detection if specified
    if args.platform:
        platform_type = args.platform
    else:
        platform_type = detect_platform()
    
    # Override config if specified
    if args.config:
        config_file = args.config
        example_file = get_config_and_example(platform_type)[1]
    else:
        config_file, example_file = get_config_and_example(platform_type)
    
    print_platform_info(platform_type, config_file, example_file)
    
    # Check dependencies
    missing_deps = check_dependencies(platform_type)
    
    if missing_deps:
        print_setup_instructions(platform_type, missing_deps)
        return 1
    
    if args.check_only:
        print("âœ… All dependencies available!")
        return 0
        
    if args.start_docker:
        print("ğŸ”§ Starting Docker services with Debug UI...")
        if start_docker_services(platform_type):
            print("âœ… Docker services started successfully!")
            print("ğŸ” Check status with: docker-compose ps")
            print("")
            
            # Start debug UI server
            from core.apps.debug_ui import DebugUIServer
            debug_port = 8080
            print(f"ğŸ› Starting Debug UI on http://localhost:{debug_port}")
            print("ğŸ’¡ Debug UI will show live data when MaestroCat agent connects")
            print("ğŸ”„ Leave this running and start the agent in another terminal")
            print("")
            print("Press Ctrl+C to stop")
            
            try:
                server = DebugUIServer(port=debug_port)
                return asyncio.run(server.start())
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Debug UI shutting down...")
                return 0
        else:
            print("âŒ Failed to start Docker services")
            return 1
    
    # Run the agent
    return asyncio.run(main())

if __name__ == "__main__":
    exit_code = cli()
    sys.exit(exit_code)
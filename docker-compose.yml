# docker-compose.yml
services:
  # WhisperLive STT Server
  whisperlive:
    image: ghcr.io/collabora/whisperlive-gpu:latest
    container_name: whisperlive
    ports:
      - "9090:9090"
    volumes:
      - whisper_models:/root/.cache/whisper
    environment:
      - OMP_NUM_THREADS=4
    command: >
      python3 run_server.py 
      --port 9090 
      --backend faster_whisper
      --omp_num_threads 4
    networks:
      - maestrocat-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import websockets; import asyncio; asyncio.run(websockets.connect('ws://localhost:9090'))"]
      interval: 30s
      timeout: 10s
      retries: 3

  kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:latest   # Replace with actual image
    container_name: kokoro
    ports:
      - "5000:5000"
    volumes:
      - kokoro_models:/models
    environment:
      - MODEL_PATH=/models
      - DEFAULT_VOICE=af_bella
      - DEVICE=cuda  # or cuda if GPU available
    networks:
      - maestrocat-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - maestrocat-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Redis for distributed event bus (optional)
  redis:
    image: redis:7-alpine
    container_name: maestrocat-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - maestrocat-network
    command: redis-server --appendonly yes

networks:
  maestrocat-network:
    driver: bridge

volumes:
  whisper_models:
  kokoro_models:
  ollama_data:
    external:
      name: ollama
  redis_data:
  prometheus_data:
